import os
import re
import torch
from PIL import Image
from transformers import (
    Qwen3VLForConditionalGeneration,
    AutoProcessor,
    BitsAndBytesConfig,
    CLIPModel,
    CLIPProcessor
)

# ----------------------------
# Qwen3-VL å€™é€‰ç”Ÿæˆå™¨
# ----------------------------
class Qwen3VLCandidateGenerator:
    def __init__(self, model_path):
        print("æ­£åœ¨åŠ è½½ Qwen3-VL æ¨¡å‹ï¼ˆ4-bit é‡åŒ–ï¼‰...")
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
        )
        max_memory = {0: "3.5GiB", "cpu": "16GiB"}
        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            quantization_config=quant_config,
            device_map="auto",
            max_memory=max_memory,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            offload_folder="D:/qwen3_deploy/offload"
        )
        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        print("âœ… Qwen3-VL åŠ è½½æˆåŠŸï¼")

    def generate_candidates(self, image_path, question, num_beams=3, max_new_tokens=20):
        image = Image.open(image_path).convert("RGB")
        image.thumbnail((384, 384))
        
        concise_question = f"{question} Answer with only the key information, no explanation."
        
        messages = [{
            "role": "user",
            "content": [{"type": "image", "image": image}, {"type": "text", "text": concise_question}]
        }]
        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.processor(text=[text], images=[image], return_tensors="pt")
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                num_beams=num_beams,
                num_return_sequences=num_beams,
                output_scores=True,
                return_dict_in_generate=True,
                early_stopping=True,
                pad_token_id=self.processor.tokenizer.pad_token_id,
                eos_token_id=self.processor.tokenizer.eos_token_id
            )

        input_len = inputs["input_ids"].shape[1]
        sequences = out.sequences
        scores = out.sequences_scores

        candidates = []
        probs = torch.softmax(scores, dim=0).tolist()
        for i in range(num_beams):
            gen_ids = sequences[i][input_len:]
            answer = self.processor.decode(gen_ids, skip_special_tokens=True).strip()
            
            if answer.lower().startswith(("the answer is", "answer:", "it is", "the brand is", "the number is")):
                match = re.search(r'(?:is|:)\s*(.+)', answer, re.IGNORECASE)
                if match:
                    answer = match.group(1).strip().rstrip('.,')
            if not answer:
                answer = "unknown"
                
            candidates.append({"text": answer, "qwen_prob": probs[i]})
        return candidates

# ----------------------------
# CLIP é‡æ’åºå™¨
# ----------------------------
def min_max_norm(scores):
    if len(scores) == 1:
        return [1.0]
    min_s, max_s = min(scores), max(scores)
    if max_s == min_s:
        return [1.0] * len(scores)
    return [(s - min_s) / (max_s - min_s) for s in scores]

class CLIPReranker:
    def __init__(self, model_path):
        print("æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = CLIPModel.from_pretrained(model_path).to(self.device)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        print("âœ… CLIP åŠ è½½æˆåŠŸï¼")

    def compute_similarity(self, image_path, candidate_texts):
        image = Image.open(image_path).convert("RGB")
        inputs = self.processor(
            text=candidate_texts,
            images=image,
            return_tensors="pt",
            padding=True
        ).to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits_per_image = outputs.logits_per_image.squeeze(0).tolist()
        return logits_per_image

# ----------------------------
# ä¸»å‡½æ•°ï¼šäº¤äº’å¼è¾“å…¥
# ----------------------------
def main():
    # ====== é…ç½®ä½ çš„æ¨¡å‹è·¯å¾„ ======
    QWEN_MODEL_PATH = "D:/qwen3_deploy/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17"
    CLIP_MODEL_PATH = "D:\\qwen3_deploy\\models--openai--clip-vit-base-patch32\\snapshots\\c237dc49a33fc61debc9276459120b7eac67e7ef"
    
    # å¯è°ƒå‚æ•°
    NUM_BEAMS = 3
    MAX_NEW_TOKENS = 50
    W1, W2 = 0.8, 0.2  # Qwen æƒé‡, CLIP æƒé‡

    print("ğŸš€ æ¬¢è¿ä½¿ç”¨ VQA äº¤äº’å¼ Demoï¼")
    print("è¯·è¾“å…¥ä»¥ä¸‹ä¿¡æ¯ï¼š\n")

    # 1. è¾“å…¥å›¾ç‰‡è·¯å¾„
    while True:
        image_path = input("ğŸ“ è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ï¼ˆä¾‹å¦‚: ./images/0.jpgï¼‰: ").strip().strip('"\'')
        if os.path.exists(image_path):
            break
        else:
            print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚\n")

    # 2. è¾“å…¥é—®é¢˜
    question = input("â“ è¯·è¾“å…¥é—®é¢˜ï¼ˆä¾‹å¦‚: what is the brand of this camera?ï¼‰: ").strip()
    if not question:
        question = "What is in the image?"

    # 3. åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶åŠ è½½ï¼‰
    print("\nâ³ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ï¼ˆé¦–æ¬¡åŠ è½½è¾ƒæ…¢ï¼‰...")
    qwen_gen = Qwen3VLCandidateGenerator(QWEN_MODEL_PATH)
    clip_reranker = CLIPReranker(CLIP_MODEL_PATH)

    # 4. æ¨ç†
    print("\nğŸ” æ­£åœ¨ç”Ÿæˆå€™é€‰ç­”æ¡ˆ...")
    candidates = qwen_gen.generate_candidates(
        image_path, 
        question, 
        num_beams=NUM_BEAMS,
        max_new_tokens=MAX_NEW_TOKENS
    )
    candidate_texts = [cand["text"] for cand in candidates]

    print("ğŸ”„ æ­£åœ¨ç”¨ CLIP é‡æ’åº...")
    clip_sims = clip_reranker.compute_similarity(image_path, candidate_texts)

    # 5. èåˆæ‰“åˆ†
    qwen_probs = [cand["qwen_prob"] for cand in candidates]
    norm_qwen = min_max_norm(qwen_probs)
    norm_clip = min_max_norm(clip_sims)
    final_scores = [
        W1 * norm_qwen[i] + W2 * norm_clip[i]
        for i in range(len(candidates))
    ]

    best_idx = final_scores.index(max(final_scores))
    final_answer = candidates[best_idx]["text"]

    # 6. è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("âœ… æ¨ç†å®Œæˆï¼")
    print(f"ğŸ“¸ å›¾ç‰‡: {image_path}")
    print(f"â“ é—®é¢˜: {question}")
    print(f"\nğŸŸ¢ Qwen æœ€é«˜æ¦‚ç‡å›ç­”: {candidates[0]['text']}")
    print(f"ğŸŸ£ CLIP å¢å¼ºåæœ€ç»ˆç­”æ¡ˆ: {final_answer}")

    print("\nğŸ“‹ å€™é€‰ç­”æ¡ˆè¯¦æƒ…:")
    for i, cand in enumerate(candidates):
        mark = " â† æœ€ä½³" if i == best_idx else ""
        print(f"  {i+1}. \"{cand['text']}\"")
        print(f"      Qwen Prob: {cand['qwen_prob']:.4f}")
        print(f"      CLIP Sim:  {clip_sims[i]:.4f}")
        print(f"      Final Score: {final_scores[i]:.4f}{mark}")
    print("="*60)

if __name__ == "__main__":
    main()